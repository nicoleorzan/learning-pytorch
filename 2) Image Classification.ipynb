{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = MNIST(root=\"data/\", download=True)\n",
    "#len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL Image: <PIL.Image.Image image mode=L size=28x28 at 0x7F5E6F2EDA30>\n",
      "Tensor Image torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "ds = FashionMNIST(root=\"data/\")\n",
    "img, label = ds[0]\n",
    "print(\"PIL Image:\", img)\n",
    "\n",
    "ds = FashionMNIST(root=\"data/\", transform=transforms.ToTensor())\n",
    "img, label = ds[0]\n",
    "print(\"Tensor Image\",img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5e6aa0cf40>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = ds[0]\n",
    "plt.imshow(img[0,:,:]) # or img.squeeze() we have to specify 0 because mtplotlib does not expect the channel idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FashionMNIST(root=\"data/\", train=True, transform=transforms.ToTensor())\n",
    "test_ds = FashionMNIST(root=\"data/\", train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random train-validation split\n",
    "\n",
    "In MNIST there are 60000 trrain imgs and 10000 test ones. We just need to create the validation ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(ds, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.weight.shape, model.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to flatten the images because nn.inear expects a vector as input, so we flatten as 1x28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        xb = x_batch.reshape(-1,784) \n",
    "        # -1 allows to generalize the model, working with any different batch size\n",
    "        out = self.linear(xb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 5.2794e-03,  8.6137e-05, -2.5085e-02,  ..., -4.6306e-03,\n",
      "          2.7690e-02,  3.0781e-02],\n",
      "        [ 2.2380e-02, -3.0241e-02,  4.5789e-03,  ...,  2.1329e-02,\n",
      "          3.3203e-03, -2.7799e-02],\n",
      "        [-7.3639e-04,  3.0775e-02,  9.3632e-03,  ..., -8.0527e-03,\n",
      "          2.7964e-02, -3.2194e-02],\n",
      "        ...,\n",
      "        [ 2.4584e-03, -1.7487e-02,  2.1221e-02,  ...,  1.7433e-02,\n",
      "         -3.2329e-02,  6.0129e-03],\n",
      "        [ 1.9399e-02, -1.5595e-02, -2.9017e-02,  ...,  2.7810e-02,\n",
      "          9.0547e-03, -3.0796e-02],\n",
      "        [-2.7791e-02,  3.0829e-02,  3.0640e-02,  ..., -1.3466e-02,\n",
      "         -3.2816e-02,  3.3246e-02]], requires_grad=True) Parameter containing:\n",
      "tensor([ 0.0306, -0.0206,  0.0217,  0.0319, -0.0275, -0.0227, -0.0011,  0.0151,\n",
      "        -0.0042, -0.0355], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 5.2794e-03,  8.6137e-05, -2.5085e-02,  ..., -4.6306e-03,\n",
       "           2.7690e-02,  3.0781e-02],\n",
       "         [ 2.2380e-02, -3.0241e-02,  4.5789e-03,  ...,  2.1329e-02,\n",
       "           3.3203e-03, -2.7799e-02],\n",
       "         [-7.3639e-04,  3.0775e-02,  9.3632e-03,  ..., -8.0527e-03,\n",
       "           2.7964e-02, -3.2194e-02],\n",
       "         ...,\n",
       "         [ 2.4584e-03, -1.7487e-02,  2.1221e-02,  ...,  1.7433e-02,\n",
       "          -3.2329e-02,  6.0129e-03],\n",
       "         [ 1.9399e-02, -1.5595e-02, -2.9017e-02,  ...,  2.7810e-02,\n",
       "           9.0547e-03, -3.0796e-02],\n",
       "         [-2.7791e-02,  3.0829e-02,  3.0640e-02,  ..., -1.3466e-02,\n",
       "          -3.2816e-02,  3.3246e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0306, -0.0206,  0.0217,  0.0319, -0.0275, -0.0227, -0.0011,  0.0151,\n",
       "         -0.0042, -0.0355], requires_grad=True)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight, model.linear.bias)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "tensor([-0.1651, -0.3735,  0.0430, -0.2147, -0.2976, -0.0973,  0.4743, -0.2808,\n",
      "        -0.0032,  0.2612], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in train_loader:\n",
    "    out = model(imgs)\n",
    "    print(out.shape)\n",
    "    print(out[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 2, 8, 5, 6, 6, 6, 6, 2, 3, 2, 2, 6, 6, 6, 1, 6, 5, 2, 2, 6, 6, 6,\n",
       "        6, 6, 6, 2, 6, 6, 6, 0, 3, 6, 6, 2, 6, 5, 6, 6, 8, 8, 6, 1, 8, 2, 2, 6,\n",
       "        6, 6, 6, 2, 6, 2, 6, 1, 8, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 6, 8, 2, 8, 8,\n",
       "        2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 6, 6, 2, 2, 6, 6, 6, 3, 2, 6, 2, 2,\n",
       "        2, 2, 2, 6])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "probs = F.softmax(out, dim=1)\n",
    "maxs_probs, preds = torch.max(probs, dim=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0900)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "torch.sum(labels == preds)/len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use cross entropy as metric. To ge the overall loss we ake an average of the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13489543\n"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy\n",
    "loss(out, preds)\n",
    "print(np.exp(-loss(out, preds).detach().numpy() )) \n",
    "# the predicted probability of the correct label, on average. Opposite of log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(xb, yb, model, loss_fn, metric=None, opt = None):\n",
    "    \n",
    "    out = model(xb)\n",
    "    loss = loss_fn(out, yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    metric_res = None\n",
    "    if metric is not None:\n",
    "        metric_res = metric(out, yb)\n",
    "        \n",
    "    return loss.item(), len(xb), metric_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = [loss_batch(xb, yb, model, loss_fn, metric=metric, opt=None) \n",
    "                   for xb, yb in valid_dl]\n",
    "        losses, nums, metrics = zip(*results)\n",
    "        total = np.sum(nums)\n",
    "        avg_losses = np.sum(np.multiply(losses, nums)) / total\n",
    "        \n",
    "        avg_metric = None\n",
    "        if metric is not None:\n",
    "            avg_metric = np.sum(np.multiply(metrics, nums)) / total\n",
    "        \n",
    "        return avg_losses, total, avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    _, pred = torch.max(out, dim=1)\n",
    "    accuracy = torch.sum(pred == labels).item()/len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3350609064102175 10000 0.149\n"
     ]
    }
   ],
   "source": [
    "model = MnistModel(input_size, num_classes)\n",
    "loss_fn = F.cross_entropy\n",
    "valid_dl = val_loader\n",
    "train_dl = train_loader\n",
    "\n",
    "val_loss, total, val_acc = evaluate(model, loss_fn, valid_dl, metric=accuracy)\n",
    "print(val_loss, total, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric=None):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for xb, yb in train_dl:\n",
    "            # train step\n",
    "            train_loss, _, _ = loss_batch(xb, yb, model, loss_fn, metric, opt)\n",
    "            # evaluation step\n",
    "            avg_eval_loss, total, avg_metric = evaluate(model, loss_fn, valid_dl, metric)\n",
    "            \n",
    "        if metric is not None:\n",
    "            print(\"Epoch=\", epoch, \", accuracy res:\", avg_metric, \", train loss=\", train_loss, \", evaluation loss=\", avg_metric)\n",
    "        else:\n",
    "            print(\"Epoch=\", epoch, \", train loss=\", train_loss, \", evaluation loss=\", avg_metric)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FashionMNIST(root=\"data/\", train=True, transform=transforms.ToTensor())\n",
    "test_ds = FashionMNIST(root=\"data/\", train=False, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)\n",
    "\n",
    "epochs = 5\n",
    "model = MnistModel(input_size, num_classes)\n",
    "loss_fn = F.cross_entropy\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "valid_dl = val_loader\n",
    "train_dl = train_loader\n",
    "\n",
    "#fit(epochs, model, loss_fn, opt, train_dl, valid_dl, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FashionMNIST(root='data/', \n",
    "                     train=False,\n",
    "                     transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    #define img as a batch\n",
    "    xb = img.unsqueeze(0)\n",
    "    print(xb.shape)\n",
    "    out = model(xb)\n",
    "    _, preds = torch.max(out, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "label= 0 Predicted= 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[284]\n",
    "plt.imshow(img[0])\n",
    "print(\"label=\", label, \"Predicted=\", predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0004,  0.0123, -0.0127,  ..., -0.0247,  0.0216, -0.0147],\n",
       "                      [ 0.0122, -0.0323,  0.0295,  ..., -0.0084, -0.0263, -0.0296],\n",
       "                      [ 0.0041,  0.0302,  0.0094,  ...,  0.0249,  0.0173, -0.0206],\n",
       "                      ...,\n",
       "                      [ 0.0167, -0.0113, -0.0267,  ..., -0.0226, -0.0150, -0.0221],\n",
       "                      [-0.0220, -0.0332,  0.0005,  ...,  0.0088, -0.0021, -0.0335],\n",
       "                      [ 0.0018, -0.0237, -0.0351,  ..., -0.0280,  0.0065,  0.0142]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0311, -0.0183,  0.0263, -0.0357, -0.0266, -0.0345, -0.0041, -0.0106,\n",
       "                       0.0043,  0.0186]))])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0004,  0.0123, -0.0127,  ..., -0.0247,  0.0216, -0.0147],\n",
       "                      [ 0.0122, -0.0323,  0.0295,  ..., -0.0084, -0.0263, -0.0296],\n",
       "                      [ 0.0041,  0.0302,  0.0094,  ...,  0.0249,  0.0173, -0.0206],\n",
       "                      ...,\n",
       "                      [ 0.0167, -0.0113, -0.0267,  ..., -0.0226, -0.0150, -0.0221],\n",
       "                      [-0.0220, -0.0332,  0.0005,  ...,  0.0088, -0.0021, -0.0335],\n",
       "                      [ 0.0018, -0.0237, -0.0351,  ..., -0.0280,  0.0065,  0.0142]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0311, -0.0183,  0.0263, -0.0357, -0.0266, -0.0345, -0.0041, -0.0106,\n",
       "                       0.0043,  0.0186]))])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "model2 = MnistModel(input_size, num_classes)\n",
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
    "model2.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
